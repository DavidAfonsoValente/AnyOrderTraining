# Any-Order Masked Training - Quick Start

Welcome! This project implements Any-Order Masked Training for trajectory-level learning in LLM-based agents.

## ğŸš€ Get Started in 5 Minutes

```bash
# 1. Run quickstart script
bash quickstart.sh

# That's it! The script will:
#   - Install dependencies
#   - Generate test data
#   - Run a short training
#   - Verify everything works
```

## ğŸ“ What You Have

```
any-order-training/
â”œâ”€â”€ README.md                    # Full project documentation
â”œâ”€â”€ PROJECT_STATUS.md            # Current status & next steps  â­ START HERE
â”œâ”€â”€ EXPERIMENTS.md               # Ablation study plans
â”œâ”€â”€ TESTING.md                   # Testing procedures
â”œâ”€â”€ quickstart.sh                # One-command setup
â”œâ”€â”€ configs/                     # Experiment configurations
â”‚   â”œâ”€â”€ base_config.yaml         # Base configuration
â”‚   â””â”€â”€ experiments/             # Experiment-specific configs
â”‚       â”œâ”€â”€ cell_masking.yaml
â”‚       â”œâ”€â”€ attribute_masking.yaml
â”‚       â””â”€â”€ scheduled_masking.yaml
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ data/                    # Data loading & processing
â”‚   â”œâ”€â”€ masking/                 # Masking strategies (core contribution!)
â”‚   â”œâ”€â”€ models/                  # Model wrappers
â”‚   â”œâ”€â”€ training/                # Training logic
â”‚   â””â”€â”€ evaluation/              # Evaluation metrics
â”œâ”€â”€ scripts/                     # Executable scripts
â”‚   â”œâ”€â”€ generate_trajectories.py # Generate training data
â”‚   â”œâ”€â”€ train.py                 # Main training script
â”‚   â”œâ”€â”€ evaluate.py              # Evaluation script
â”‚   â””â”€â”€ run_ablations.sh         # Run full ablation study
â””â”€â”€ data/                        # Data directory (created by scripts)
```

## ğŸ¯ Current Status

âœ… **Ready to Use:**
- Complete data pipeline
- All 3 masking strategies (cell, attribute, scheduled)
- Training infrastructure
- Evaluation suite
- Ablation experiment scripts

ğŸš§ **Next Step:**
- Integrate with real LLaDA2.0 model (currently using mock model)

## ğŸ“– Key Documents

1. **PROJECT_STATUS.md** â­ - Read this first!
   - Shows what's done vs what's needed
   - Clear integration checklist
   - Step-by-step guide for LLaDA2.0 integration

2. **EXPERIMENTS.md** - Ablation study plans
   - 4 detailed experiments
   - Expected results
   - Analysis procedures

3. **TESTING.md** - Verification guide
   - Unit tests
   - Integration tests
   - Troubleshooting

## ğŸ”¬ Quick Experiment

```bash
# Generate data (1200 trajectories, ~5 min)
python scripts/generate_trajectories.py \
    --env BabyAI-GoToRedBall-v0 \
    --num_episodes 1200 \
    --output_dir data/raw

# Train with cell-level masking (30% probability)
python scripts/train.py \
    --config configs/experiments/cell_masking.yaml \
    --output_dir outputs/exp1

# Evaluate
python scripts/evaluate.py \
    --checkpoint outputs/exp1/checkpoints/best.pt \
    --metric all
```

## ğŸ¨ Masking Strategies Implemented

### 1. Cell-Level Masking
```
[cell_obj, cell_color, cell_state] â†’ [MASK, MASK, MASK]
```
- Masks all 3 attributes together
- Teaches spatial reasoning & object permanence
- Config: `configs/experiments/cell_masking.yaml`

### 2. Attribute-Level Masking
```
[door, red, MASK] â†’ predict state (open/closed)
```
- Masks individual attributes
- Teaches dynamics, affordances, causal structure
- Config: `configs/experiments/attribute_masking.yaml`

### 3. Scheduled Masking
```
Probability: 0.15 â†’ 0.30 â†’ 0.50 (gradually increases)
```
- Curriculum learning approach
- Config: `configs/experiments/scheduled_masking.yaml`

## ğŸƒ Run Full Ablation Study

```bash
# Runs all experiments automatically
bash scripts/run_ablations.sh

# Results will be in:
#   outputs/ablations/RESULTS_SUMMARY.md
```

## ğŸ’¡ What Makes This Special

**Any-Order Masking**: 
- Traditional SFT: Fixed order (past â†’ predict next action)
- This project: Random masks each epoch (any element can be masked)
- Benefits: Learns richer representations, better generalization

**Key Innovation**:
- Single-pass reconstruction (no multi-step diffusion)
- Works with masked DLMs like LLaDA2.0
- Subsumes standard SFT as special case

## ğŸ› ï¸ Integration with LLaDA2.0

See `PROJECT_STATUS.md` for detailed integration guide.

Quick summary:
1. Download LLaDA2.0 model
2. Replace mock model in `scripts/train.py`
3. Create tokenization layer
4. Run experiments!

## ğŸ“Š Expected Results

With mock model (testing only):
- âœ… Pipeline works end-to-end
- âœ… Training converges
- âœ… Metrics computed

With real LLaDA2.0:
- ğŸ¯ Observation accuracy > 80%
- ğŸ¯ Action accuracy > 60%
- ğŸ¯ Task success > 40%

## ğŸ†˜ Need Help?

1. **Something not working?** â†’ Check `TESTING.md`
2. **Want to run experiments?** â†’ See `EXPERIMENTS.md`
3. **Ready to integrate LLaDA2.0?** â†’ Read `PROJECT_STATUS.md`
4. **General questions?** â†’ See `README.md`

## ğŸ“ Citation

```bibtex
@misc{anyorder2025,
  title={Any-Order Masked Training for Trajectory-Level Learning in LLM-Based Agents},
  author={Your Name},
  year={2025}
}
```

## ğŸ‰ You're Ready!

The project is fully implemented and tested. Next step: integrate with LLaDA2.0 and run the experiments!

**Start here:** `PROJECT_STATUS.md` â†’ Integration Checklist
